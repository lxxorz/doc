



在二维空间中，所有的向量都可以用 $\vec{v}=(a\hat{i}, b\hat{j})$ 都能通过基向量的「缩放」「平移」的线性变换然后相加得到。也就是 $\vec{v}=a\hat{i}+b\hat{j}$。非常自然的可以联想到我们让两个向量相加实际上就是让他们的基向量进行相加。
$$
\begin{align}
\vec{u}&=a\hat{i}+b\hat{j} \\
\vec{v}&=c\hat{i}+b\hat{j} \\
\vec{u}+\vec{v}&=(a+c)\hat{i}+(b+d)\hat{j}
\end{align}
$$
写成矩阵的形式就是 $$
\begin{pmatrix}
a & b
\end{pmatrix}\begin{pmatrix}
\hat{i}  \\
\hat{j}
\end{pmatrix} + \begin{pmatrix}
c & d
\end{pmatrix}\begin{pmatrix}
\hat{i} \\
\hat{j}
\end{pmatrix}=\begin{pmatrix}
a+c & b+d 
\end{pmatrix}\begin{pmatrix}
\hat{i} \\
\hat{j}
\end{pmatrix}
=\begin{pmatrix} 
 a\hat{i}+c\hat{i} \\
b\hat{j}+d\hat{j}
\end{pmatrix}
$$
如果我们从基向量 $\hat{i},\hat{j}$ 的角度来看实际实际上实在对他们做变换 $\hat{i}\to \hat{i}(a+c)$ 和 $\hat{j}\to \hat{j}(b+d)$。

[[数学康复训练/线性代数/deab34003aeebb6d7f99261a041b8a5f_MD5.png|Open file:]]
![[数学康复训练/线性代数/deab34003aeebb6d7f99261a041b8a5f_MD5.png]]

